
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}
\bibliographystyle{splncs03}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\usepackage{color}
\usepackage{url}
\usepackage{amsmath}
\usepackage[colorinlistoftodos,prependcaption,textsize=small]{todonotes}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Replication with variable consistency and tolerance to arbitrary uncorrelated faults}
% a short form should be given in case it is too long for the running head
\titlerunning{Replication with variable consistency and tolerance to arbitrary uncorrelated faults}


\author{Miguel Pires \\
\texttt{miguel.pires@tecnico.ulisboa.pt}}
\authorrunning{Miguel Pires}

% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Instituto Superior Técnico (Campus Taguspark) \\
Av. Prof. Doutor Aníbal Cavaco Silva \\
2744-016 Porto Salvo - Portugal}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
The abstract should summarize the contents of the paper and should
contain at least 70 and at most 150 words. It should be written using the
\emph{abstract} environment.
\keywords{A few key words}
\end{abstract}

\section{Introduction}

\section{Related Work}

Due to the specific characteristics of the scenario for which we try to optimize, there are several areas that contain works of interest to us. Subsection \ref{Paxos} describes previous work regarding consensus and several Paxos variants. Subsection \ref{Non-Crash} discusses non-crash fault models as well as protocols for these environments. Subsection \ref{Partial Synchrony} describes different partial synchrony models and how they fit in the synchrony spectrum. Subsection \ref{Variable Consistency} discusses variable consistency models that attempt to strike a balance between strong and weak consistency in order to obtain advantages from both.

\subsection{Paxos and its Variants} \label{Paxos}

The Paxos protocol family solves consensus by circumventing the well-known FLP impossibility result \cite{Fischer1985}. It does this by making the observation that, while consensus is unsolvable in asynchronous systems, a system is usually synchronous since delays are sporadic and temporary. Therefore, as long as consistency is guaranteed regardless of synchrony, Paxos can forgo of progress during the temporary periods of asynchrony. Using this insight, Paxos ensures consistency even when the system is asynchronous but can only guarantee progress while it is synchronous and no more than $f$ faults occur for a system of $2f+1$ replicas \cite{Lamport2001}. The classic form of Paxos employs a set of proposers, acceptors and learners and runs in a sequence of ballots. To ensure progress during synchronous periods, proposals are serialized by a distinguished proposer, the leader. It is the leader that gathers proposals from other proposers and requests, in a \textit{phase 1a} message, that acceptors promise to not accept any ballots smaller than its own. Each acceptor responds to the leader's request, in a \textit{phase 1b} message, with either a promise or a message stating that it has already sent a promise to a proposer with a higher-numbered ballot. If the leader manages to obtain a promise from a quorum of acceptors, then it sends a \textit{phase 2a} message containing its proposal. The acceptors may accept the proposal and inform the learners with a \textit{phase 2b} message, if another proposer hasn't requested a promise for a higher-numbered ballot in the meanwhile. The fact that any proposer can request a promise and prevent lower ballots from being completed is the reason why Paxos can only ensure progress in a synchronous environment. Some proposer must be able to establish itself as the only leader in order to prevent proposers from continuously requesting promises for ballots with higher numbers. \par
We now describe the form in which Paxos is most commonly deployed, Multi(Decree)-Paxos. Multi-Paxos provides an optimization of the basic Paxos' message pattern by omitting the first phase of messages from all but the first ballot for each leader \cite{Renesse2011}. This means that a leader only needs to send a \textit{prepare} message once and subsequent proposals may be sent directly in \textit{phase 2a} messages, without requesting a promise from the acceptors. This reduces the message pattern in the common case from five message delays to just three (from proposal to learning). Since there are no implications on the quorum size or guarantees provided by Paxos, the reduced latency comes at no additional cost. \par
Fast Paxos observes that it's possible to improve on previous results by allowing proposers to propose values directly to acceptors. To this end, the protocol distinguishes between fast and classic ballots, where fast ballots use $3f+1$ acceptors instead of the usual $2f+1$ \cite{Lamport2006}. The optimized scenario occurs during fast ballots, in which only two messages broadcasts are necessary: \textit{phase 2a} messages between a proposer and the acceptors, and \textit{phase 2b} messages between acceptors and learners. This creates the possibility of two proposers concurrently proposing values to the acceptors and generating a conflict. The cost of dealing with this conflict depends on the chosen method of recovery, coordinated or uncoordinated, and if the leader or acceptors are also learners. The leader may choose  the recovery method in the beginning of the ballot. If coordinated recovery is chosen in a ballot $i$, then the cost is a \textit{phase 2} of round $i+1$ (i.e., two additional message delays). If uncoordinated recovery is chosen then the cost is a single \textit{phase 2b} message in round $i+1$ (i.e., one message delay) \cite{Lamport2006}. These numbers assume that either the leader (coordinated recovery) or the acceptors (uncoordinated recovery) are learners, which is a reasonable assumption. \par
Generalized Paxos addresses Fast Paxos' shortcomings regarding collisions. More precisely, it allows acceptors to accept different sequences of commands as long as non-commutative operations are totally ordered \cite{Lamport2005}.  Non-commutativity between operations is generically represented as a dependence relation. Generalized Paxos abstracts the traditional consensus problem of agreeing on a single value to the problem of agreeing on an increasing set of values. \textit{C-structs} provide this increasing sequence abstraction and allow us to define different consensus problems. If we define the sequence of learned commands of a learner $l_i$ as a \textit{c-struct} $learned[l_i]$, then the consistency requirement for consensus can be defined as:\par
\todo{Should this be inlined or can it stay like this? I was already practically quoting it and it looked odd}\textbf{Consistency} $learned[l_1]$ and $learned[l_2]$ are always compatible, for all learners $l_1$ and $l_2$. \par
For two \textit{c-structs} to be compatible, they must have a \textit{common upper bound}. Plainly spoken this means that, for any two \textit{c-structs} such as $learned[l_1]$ and $learned[l_2]$, there must exist some \textit{c-struct} \todo{Should I give an example of what we mean by extension and how non-commutative commands prevent an extension?}that extends them. Defining \textit{c-structs} as command histories enables acceptors to agree on different sequences of commands and still preserve consistency as long as dependence relationships are not violated. This means that commutative commands can be ordered differently regarding each other but interfering commands must preserve the same order across each sequence at any learner. This guarantees that solving the consensus problem for histories is enough to implement a state-machine replicated system. \par
Mencius is also a variant of Paxos that tries to address the bottleneck of having a single leader through which every proposal must go through. In Mencius, the leader of each round rotates between every process. The leader of round $i$ is the process $p_k$, such that $k = n mod\ i$.  Leaders with nothing to propose can skip their turn by proposing a \textit{no-op}. If a leader is slow or faulty, the other replicas can execute \textit{phase 1} to revoke the leader's right to propose a value but they can only propose a \textit{no-op} instead \cite{Mao2008}. Considering that non-leader replicas can only propose \textit{no-ops}, a \textit{no-op} command from the leader can be accepted in a single message delay since there is no chance of another value being accepted. If some non-leader server revokes the leader's right to propose and suggests a \textit{no-op}, then the leader can still suggest a value $v \neq$ \textit{no-op} that will eventually be accepted as long as $l$ isn't permanently suspected. The usage of an unreliable failure detector is enough to guarantee that eventually all faulty processes, and only those, will be suspected. Mencius also takes advantage of commutativity by allowing out-of-order commits, where values $x$ and $y$ can be learned in different orders by different learners if there isn't a dependence relationship between them. Experimental evaluation confirms that Mencius is able of scaling to a higher throughput than Paxos. \todo{Should I mention EPaxos?}

\subsection{Non-crash Fault Models} \label{Non-Crash}

Non-crash fault models emerged to cope with the effect of malicious attacks and software errors. These models (e.g., the arbitrary fault model) assume a stronger adversary than previous crash fault models. The Byzantine Generals Problem is defined as a set of Byzantine generals that are camped in the outskirts of an enemy city and have to coordinate an attack. Each general can either decide to attack or retreat and there may be $f$ traitors among the generals that try to prevent the loyal generals from agreeing on the same action. The problem is solved if every loyal general agrees on what action to take \cite{Lamport1982}. Like the traitorous generals, the Byzantine adversary is one that may force a faulty replica to display arbitrary behaviour and even coordinate multiple faulty replicas in an attack. \par
PBFT is a protocol that solves consensus for state machine replication (SMR) while tolerating up to $f$ Byzantine faults \cite{Castro1999}. The system moves through configurations called \textit{views} in which one replica is the primary and the remaining replicas are the backups. The safety property of the algorithm requires that operations be totally ordered. The protocol starts when a client sends a request for an operation to the primary, which in turn assigns a sequence number to the request and multicasts a \textit{pre-prepare} message to the backups. This message contains the timestamp, the message's digest, the view and the actual message. If a backup replica accepts the pre-prepare message, after verifying that the view number and timestamp are correct, it multicasts a \textit{prepare} message to and adds both messages to its log. The prepare message is similar to the pre-prepare message except that it doesn't contain the client's request message. Both of these phases are needed to ensure that the requested operation is totally ordered at every correct replica. The protocol's safety property requires that the replicated service must satisfy linearizability, and therefore operations must be totally ordered. After receiving $2f$ prepare messages, a replica multicasts a \textit{commit} message and commits the message to its log when it has received $2f$ commit messages from other replicas. The liveness property requires that clients must eventually receive replies to their requests, provided that there are at most $\lfloor\frac{n-1}{3}\rfloor$ faults and the transmission time doesn't increase continuously. This property represents a weak liveness condition but one that is enough to circumvent Fischer, Lynch and Paterson's impossibility result \cite{Fischer1985}. A Byzantine leader may try to prevent progress by omitting pre-prepare messages when it receives operation requests from clients, but backups can trigger new views after waiting for a certain period of time. \par
The cost of $3f+1$ replicas in a PBFT system stems from the immense power given to the adversary. The cross fault tolerance (XFT) model claims that this adversary is too strong when compared to the behavior of deployed systems \cite{Liu2015}. XFT assumes that machine and network faults are uncorrelated and this assumption allows it to have a cost equal to crash fault tolerant systems, $2f+1$, while still tolerating $f$ non-crash faults. The safety condition for this model is that correct replicas commit requests in a total order, as long as the system is not in anarchy. A system is in anarchy if there are any non-crash faults $f_{nc}(s)$ and the sum of crash faults $f_c(s)$, non-crash faults $f_{nc}(s)$ and partitioned replicas $f_p(s)$ is larger than the fault threshold. That is, the system is in anarchy if $f_{nc}(s) > 0\ \land\ f_{nc}(s)+ f_c(s)+f_p(s) > f$.
Liveness is also guaranteed as long as the system is outside of anarchy. XFT replicates client requests to $f+1$ replicas synchronously while the remaining $f$ may learn about requests through lazy replication \cite{Ladin1992}. If an active replica fails, a view change is triggered. A view change requires all the replicas in the new view to collect the recent state, including the commit log, which may be substantial. This overhead can be decreased through the use of checkpointing. The protocol includes the exchange of four rounds of messages: SUSPECT, VIEW-CHANGE, VC-FINAL and NEW-VIEW. The experimental evaluation performed by the authors shows that, after each crash, the system triggers a view change that last less than 10 seconds. \par
The creators of the Visigoth Fault Tolerance (VFT) model also make the observation that the Byzantine model is unrealistic, especially for environments such as data centers. The Byzantine adversary's strength forces protocols to incur in an unnecessary cost of $3f+1$ replicas. VFT also observes that while data centers rarely observe arbitrary coordinated faults, they commonly experience data corruption faults (e.g., bit flips) \textcolor{red}{CITATION} and these also represent a type of arbitrary behaviour. To reconcile the need to tolerate different types of arbitrary faults, VFT observes that it is very unlikely for data corruption to affect the same data across multiple replicas. Therefore the key distinguishing factor between these types of arbitrary faults is \textit{correlation}. The VFT model proposes a customizable model that defines a limit of $u$ faults, that bounds the number of allowed omission (i.e., crash) and comission (i.e., arbitrary) faults, a limit of $o$ correlated comission faults, a limit of $r$ arbitrary faults and, for every process $p_i$, $s$ correct processes $p_j$ that are slow in respect it \cite{Porto2015}. These additional assumptions allow VFT to decrease the replication factor from $n \geq 2u+r+1$ (i.e., $3f+1$ in the traditional notation) to $n \geq u+s+o+1$, when $u > s$. These parameters allow us to configure the system to tolerate any combination of faults. The VFT protocol is adapted from the BFT-SMaRT protocol and contains two main message patterns: the first consists in two message steps that are executed during epoch changes to collect information about previous epochs and disseminate this information to replicas; and the second consists in two \todo{I've been using "multicast" to say "all-to-all broadcast". Is this correct?} multicast phases that are executed for each client request to implement the state machine replication. To gather a quorum, the gatherer tries to collect a larger quorum of $n-s$ replies, which can be seen as collecting replies from the fast but potentially faulty replicas. If a timeout occurs, the quorum is reduced to $n-u$ and this can be seen as collecting replies from correct but possibly slow replicas. This reduced quorum size is enough to ensure an intersection because since $x$ replicas didn't reply by the first timeout and only $s$ may be slow, then $x-s$ replicas must have crashed and won't participate in future quorums. The experimental evaluation confirms that VFT can tolerate uncorrelated arbitrary faults with resource-efficiency and performance that resembles that of CFT systems instead of BFT. \par
Dijkstra proposed self-stabilizing systems as a solution to the problem of keeping cyclic sequences of processes in a legitimate state even after the occurrence of transient faults \cite{Dijkstra1974}. In a self-stabilizing system, each process may contain several privileges and may only exchange information with its neighbors. The requirement of keeping the system in a legitimate state is met if: (1) one or more privileges are present; (2) in each state, the possible moves transfer the system to a legitimate state again (3) each privilege exists in at least one legitimate state; and (4) for any pair of legitimate states, there is a sequence of moves that transfers the system from one to the other. These systems define a set of rules for transferring privileges in a way such that a system that starts from an arbitrary state is guaranteed to converge to a legitimate state and remain in a set of legitimate states. Self-stabilizing systems are an example of a non-crash model that isn't strictly stronger than crash fault models since, due to the system's ring-like structure, a crash fault would render the system unusable. A self-stabilizing system tolerates any arbitrary corruption of the state but, unlike other non-crash models, it assumes that a process always executes correct code.\par
The Arbitrary State Corruption (ASC) model defines ASC faults as faults that occur at a process $p$ and either make it crash or assign an arbitrary value to a variable of the process's state. The assignment of an arbitrary value to a variable of $p$'s state may also alter its control flow in a way such that the process may execute any of its instructions \cite{Correia2012}. This modeling stems from three main observations: (1) most tolerable failures are caused by state corruptions instead of arbitrary or malicious behavior, (2) control-flow faults are very likely to cause incorrect outputs \cite{Kalbarczyk2003}, and (3) the relevant failures are caused by transient faults instead of permanent faults being repeatedly activated. In the context of this model, the ASC hardening technique is proposed to deal with ASC faults. This technique guarantees that if a correct hardened process $p_c$ receives a message $m$ from a faulty hardened process $p_{f1}$, then $p_c$ discards $m$ without modifying its state. If a faulty process $p_{f2}$ receives $m$ and modifies its local state, then it crashes before sending an output message. ASC hardening uses message integrity checks to prevent network corruption and state integrity checks to replicate process state. Every time a value is read from a variable, it's checked against the replicated value. If a mismatch occurs, the process crashes itself to preserve integrity. Control-flow faults are prevented through the use of gates. For each control-flow gate, two variables, an original $c$ and a replica $c'$, are created along with a specific label $L$. If both variables are set to $L$, then the process has already executed the control-flow instruction. Gates may provide \textit{at-most-once} semantics to ensure that a gate isn't executed twice even in the presence of faults or \textit{at-least-once} semantics to ensure that a gate has been executed before.\todo{Too detailed?} For an \textit{at-most-once} gate, if $c \neq c'$ or $c = L$ the process crashes itself. Otherwise, both $c$ and $c'$ are set to $L$. For an \textit{at-least-once} gate, if $c=c'=L$ the gate has been reached before. Otherwise, the process crashes itself. \textcolor{red}{Comment on evaluation. Clarification needed}

\todo{Should I mention Zeno?}
\subsection{Partial Synchrony} \label{Partial Synchrony}

Failure detectors are a formalism that allows us to abstract the assumptions a system is allowed to make about its environment. Unreliable failure detectors were proposed by Chandra and Toueg as a way to circumvent impossibility results due asynchrony. These failure detectors would output information about which processes have crashed, therefore encapsulating the necessity of determining whether a process is crashed or slow \cite{DeepakChandra1996}. The guarantees provided by the failure detector are specified in terms of \textit{completeness}, which requires that a failure detector must eventually suspect every crashed process, and \textit{accuracy}, which restricts the mistakes a failure detector can make. A class of failure detectors of particular interest is Eventually Weak failure detectors, $\Diamond\mathcal{W}$, since it is the weakest possible class. $\Diamond\mathcal{W}$ failure detectors provide the following guarantees:\par
\textbf{Completeness} Eventually every crashed process is permanently suspected by some correct process.\par
\textbf{Accuracy} Eventually some correct process is never suspected by any correct process.\par
Chandra and Toueg prove that consensus is solvable using $\Diamond\mathcal{W}$ with $f < \lceil n/2 \rceil$ \cite{DeepakChandra1996}. Chandra et al. also define a new class of failure detectors, $\Omega$-accurate failure detectors \cite{Chandra1996}. The $\Omega$ class is shown to be at least as strong as $\Diamond\mathcal{W}$ and any failure detector that solves consensus must be at least as strong as $\Omega$ and, therefore at least as strong as $\Diamond\mathcal{W}$. \par
The accuracy properties of the failure detectors proposed by Chandra and Toueg in \cite{DeepakChandra1996} apply to the entirety of system's processes. If network partitions occur, the accuracy property can violated. $\Gamma$-accurate failure detectors were proposed by Guerraoui and Schiper \cite{Guerraoui96gammaaccurate} to deal with this. These failure detectors apply the completeness and accuracy properties to a subset of the system's processes and allow consensus to be solved in a fully asynchronous model. \par
Dwork, Lynch and Stockmeyer note that one reasonable situation is when there exists an upper bound $\Delta$ on message delivery latency but it isn't known \textit{a priori} \cite{Dwork1988}. In this situation, the impossibility results shown in \cite{Fischer1985} and \todo{They also refer to this paper as well as the FLP result. Should I keep it?} \cite{Dolev1983} don't apply since communication can be considered synchronous. However, processes participating in consensus must know $\Delta$ so they can consider processes faulty after waiting for the correct amount of time. To deal with this, an additional constraint is applied to the model: there is a global stabilization time (GST) after which latency is bounded by $\Delta$. An algorithm that solves consensus is required to ensure \textit{safety} regardless of how asynchronous the system behaves, but it only needs to ensure \textit{termination} if $\Delta$ eventually holds. The authors also show that f-resilient consensus is possible in the partially synchronous model if $N \geq 2f+1$, for crash faults, and $N \geq 3f+1$, for Byzantine faults.\todo{TODO: Incomplete. Cover synchronous / asynchronous processors $\Phi$}\par
Aguilera et al. propose a partial synchrony model based on the notion of \textit{set timeliness} and show that it can be used to study problems weaker than consensus \cite{Aguilera2012}. This model generalizes the concept of \textit{process timeliness} (i.e., an upper bound $\Phi$ on the relative speeds of processes) \cite{Dwork1988} to sets of processes by considering a set of processes as a single virtual process that executes an action whenever a process in $P$ executes an action and applying the definition of process timeliness to virtual processes. A set of processes $P$ is considered to be timely with respect to another set of processes $Q$ if, for some integer $i$, every interval that contains $i$ steps in $Q$ also contains at least one step of a process in $P$. This model is used to prove possibility and impossibility results for the $f$-resilient $k$-set agreement problem. This problem is a generalization of the consensus problem where, instead of having $n$ processes that have to agree on a single value, they need to decide on at most $k$ different values \cite{Chaudhuri1993}. To solve this problem, Aguilera et al. propose a $f$-resilient $k$-anti-$\Omega$ failure detector that, for every process $p$, outputs a set $fdOutput_p$ such that there exists a correct process $c$ that eventually doesn't belong to $fdOutput_p$. Anti-$\Omega$ failure detectors were shown to be the weakest failure detectors that allow set agreement to be solved \cite{Zielinski2010}. The algorithm works for two sets $P$ and $Q$ of sizes $k$ and $f+1$, respectively, such that $P$ is timely with respect to $Q$. In this $f$-resilient $k$-anti-$\Omega$ failure detector, each process has a periodically incremented heartbeat and also timeout timers for every set $A$, such that $A$ is a subset of all processes $\Pi_n$ of size $k$. Whenever a process detects that \textit{any} process in some set $A$ incremented its heartbeat, it resets $A$'s timer. If $A$'s timer expires for a process $p$, $p$ increments its timeout value for $A$ and also increments a shared register $Counter[A,p]$ that represents $A$'s untimeliness with respect to $p$. An accusation counter of a set $A$ is the $(f+1)$-st smallest value of $Counter[A,*]$. Each process $p$ picks the set that has the smallest accusation counter as its $winnerset_p$ as sets $\Pi_n - winnerset_p$ as the output of $k$-anti-$\Omega$. Since $P$ is timely with respect to $Q$, eventually the accusation counter of $P$ stops changing and one of the sets whose accusation counter stops changing is the smallest and is picked as the winner. \par

\subsection{Variable Consistency Models} \label{Variable Consistency}
\textcolor{red}{RedBlue, Indigo, ...}

\subsection{Coordination Systems} \label{Coordination Systems}
\textcolor{red}{Zookeper } 


\bibliography{references}

\end{document}
