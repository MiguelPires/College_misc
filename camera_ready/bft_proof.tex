\vspace{-0.2cm}
\subsection{Correctness}
We now prove the correctness of the presented Byzantine Generalized Paxos protocol. Due to space constraints, we only discuss the proof of consistency, but the
remaining proofs are available in the tech report.
\vspace{-0.5cm}
\begin{table}[h!]
	\renewcommand{\arraystretch}{1.5}
	\centering
	\begin{tabularx}{\linewidth}{ |c|X|}
		\hline
		Invariant/Symbol & Definition \\
		\hline
		$\thicksim$ & Equivalence relation between sequences \\
		\hline
		$X \sqsubseteq Y$ & The sequence $X$ is a prefix of sequence $Y$ \\
		\hline
		$\mathcal{L}$ & Set of learner processes \\
		\hline
		$\mathcal{P}$ & Set of proposals (commands or sequences of commands) \\
		\hline
		$\bot$ & Empty command \\
		\hline
		$learned_{l_i}$ & Learner $l_i$'s $learned$ sequence of commands \\
		\hline
		$learned(l_i,s)$ & $learned_{l_i}$ contains the sequence $s$ \\
		\hline
		$maj\_accepted(s)$ & $N-f$ acceptors sent phase 2b messages to the learners for sequence $s$\looseness=-1 \\
		\hline
		$min\_accepted(s)$ & $f+1$ acceptors sent phase 2b messages to the learners for sequence $s$\looseness=-1 \\
		\hline
		
	\end{tabularx} 
	\vspace{\smallskipamount}
	\caption{Proof notation} 
	\label{table:1}
\end{table}
%
\vspace{-1.1cm}
\begin{theorem}
At any time and for any two correct learners $l_i$ and $l_j$, $learned_{l_i}$ and $learned_{l_j}$ can subsequently be extended to equivalent sequences \par
\end{theorem} 
\textbf{Proof:} \par
\parbox{\linewidth}{\strut1. At any given instant, $\forall s,s' \in \mathcal{P}, \forall l_i,l_j \in \mathcal{L}, learned(l_j,s) \land learned(l_i,s') \implies \exists \sigma_1,\sigma_2 \in \mathcal{P} \cup \{\bot\}, s \bullet \sigma_1 \thicksim s' \bullet \sigma_2$}  \par
\indent\indent\parbox{\linewidth}{\strut\textbf{Proof:} }\par
\indent\indent\indent\parbox{\linewidth-\algorithmicindent*3}{\strut1.1. At any given instant, $\forall s,s' \in \mathcal{P}, \forall l_i,l_j \in \mathcal{L}, learned(l_i,s) \land learned(l_j,s') \implies (maj\_accepted(s) \lor (min\_accepted(s) \land s \bullet \sigma_1 \thicksim x \bullet \sigma_2)) \land (maj\_accepted(s') \lor (min\_accepted(s') \land s' \bullet \sigma_1 \thicksim x \bullet \sigma_2)), \exists \sigma_1, \sigma_2 \in \mathcal{P} \cup \{\bot\}, \forall x \in \mathcal{P}$} \par
\indent\indent\indent\indent\parbox{\linewidth-\algorithmicindent*4}{\strut\textbf{Proof:} A sequence can only be learned if the learner gathers $N-f$ votes (i.e., $maj\_accepted(s)$) or if it is universally commutative (i.e., $s \bullet \sigma_1 \thicksim x \bullet \sigma_2,\ \exists \sigma_1, \sigma_2 \in \mathcal{P} \cup \{\bot\}, \forall x \in \mathcal{P}$) and the learner gathers $f+1$ votes (i.e., $min\_accepted(s)$).The first case includes both gathering $N-f$ votes directly from each acceptor (Algorithm \ref{BFT-Learn} lines \{1-4\}) and gathering $N-f$ proofs of vote from only one acceptor, as is the case when the sequence contains a special checkpointing command (Algorithm \ref{BFT-Learn} \{6-11\}). The second case requires that the sequence must be commutative with any other (Algorithm \ref{BFT-Learn} \{1-4\}). This is encoded in the logical }\par
\indent\indent\indent\indent\parbox{\linewidth-\algorithmicindent*4}{expression $s \bullet \sigma_1 \thicksim x \bullet \sigma_2$ which is true the if learned sequence can be extended with $\sigma_1$ to the same that any other sequence $x$ can be extended to with a possibly different sequence $\sigma_2$, therefore making it impossible to result in a conflict.}

\indent\indent\indent\parbox{\linewidth-\algorithmicindent*3}{\strut1.2. At any given instant, $\forall s,s' \in \mathcal{P}, maj\_accepted(s) \land maj\_accepted(s') \implies \exists \sigma_1,\sigma_2 \in \mathcal{P} \cup \{\bot\}, s \bullet \sigma_1 \thicksim s' \bullet \sigma_2$}\par
\indent\indent\indent\indent\parbox{\linewidth}{\strut\textbf{Proof:} Proved by contradiction.}\par
\indent\indent\indent\indent\indent\parbox{\linewidth-\algorithmicindent*5}{\strut1.2.1.~At any given instant, $\exists s,s' \in \mathcal{P}, \forall \sigma_1,\sigma_2 \in \mathcal{P} \cup \{\bot \}, maj\_accepted(s) \land maj\_accepted(s') \wedge s \bullet \sigma_1 \not\thicksim s' \bullet \sigma_2$} \par
\indent\indent\indent\indent\indent\indent\parbox{\linewidth}{\strut\textbf{Proof:} Contradiction assumption.}\par
\indent\indent\indent\indent\indent\parbox{\linewidth-\algorithmicindent*5}{\strut1.2.2. Take a pair proposals $s$ and $s'$ that meet the conditions of 1.2.1 (and are certain to exist by the previous point), then $s$ and $s'$ are non-commutative }\par
\indent\indent\indent\indent\indent\indent\parbox{\linewidth-\algorithmicindent*6}{\strut\textbf{Proof:} If $\forall \sigma_1,\sigma_2 \in \mathcal{P} \cup \{\bot\}, s \bullet \sigma_1 \not\thicksim s' \bullet \sigma_2$, then $s$ and $s'$ must contain non-commutative commands differently ordered. Otherwise, some combination of $\sigma_1$ and $\sigma_2$ would be commutative. If $s \bullet \sigma_1 \not\thicksim s' \bullet \sigma_2$ even for commutative $\sigma_1$ and $\sigma_2$ then $s$ and $s'$ must contain non-commutative commands in different relative orders.}\par
\indent\indent\indent\indent\indent\parbox{\linewidth}{\strut1.2.3. At any given instant, $\neg (maj\_accepted(s) \land maj\_accepted(s'))$ } \par
\indent\indent\indent\indent\indent\indent\parbox{\linewidth-\algorithmicindent*6}{\strut\textbf{Proof:} Since $s$ and $s'$ are non-commutative, therefore not equivalent, and each correct acceptor only votes once for a new proposal (Algorithm \ref{BFT-Acc}, lines \{31-46\}), any learner will only obtain $N-f$ votes for one of the sequences (Algorithm \ref{BFT-Learn}, lines \{1-4\}).}\par
\indent\indent\indent\indent\indent\parbox{\linewidth}{\strut1.2.4. A contradiction is found, Q.E.D. }\par
\indent\indent\indent\parbox{\linewidth-\algorithmicindent*3}{\strut1.3. For any pair of proposals $s$ and $s'$, at any given instant, $\forall x \in \mathcal{P}, \exists \sigma_1,\sigma_2,\sigma_3,\sigma_4 \in \mathcal{P} \cup \{\bot\}, (maj\_accepted(s) \lor (min\_accepted(s) \land s \bullet \sigma_1 \thicksim x \bullet \sigma_2)) \land (maj\_accepted(s') \lor (min\_accepted(s') \land s \bullet \sigma_1 \thicksim x \bullet \sigma_2)) \implies s \bullet \sigma_3 \thicksim s' \bullet \sigma_4$}\par
\indent\indent\indent\indent\parbox{\linewidth}{\strut\textbf{Proof:} By 1.2 and by definition of $s \bullet \sigma_1 \thicksim x \bullet \sigma_2$.}\par
\indent\indent\indent\parbox{\linewidth-\algorithmicindent*3}{\strut1.4. At any given instant, $\forall s,s' \in \mathcal{P}, \forall l_i,l_j \in \mathcal{L}, learned(l_i,s)\ \land\ learned(l_j,s') \implies \exists \sigma_1,\sigma_2 \in \mathcal{P} \cup \{\bot\}, s \bullet \sigma_1 \thicksim s' \bullet \sigma_2$ }\par
\indent\indent\indent\indent\parbox{\linewidth}{\strut\textbf{Proof:} By 1.1 and 1.3.}\par
\indent\indent\indent\parbox{\linewidth}{\strut1.5. Q.E.D. }\par
\parbox{\linewidth-\algorithmicindent*3}{\strut2. At any given instant, $\forall l_i,l_j \in \mathcal{L}, learned(l_j,learned_j) \land learned(l_i,learned_i) \implies \exists \sigma_1,\sigma_2 \in \mathcal{P} \cup \{\bot\}, learned_i \bullet \sigma_1 \thicksim learned_j \bullet \sigma_2$}\par
\indent\indent\parbox{\linewidth}{\strut\textbf{Proof:} By 1.}\par
\parbox{\linewidth}{\strut3. Q.E.D.} \par
