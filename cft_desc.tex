\section{Protocol} 

This section describes the crash fault tolerant version of the Generalized Paxos protocol for our simplified problem. The only modifications applied to the protocol were made to make it simpler while still ensuring its correctness. The protocol should still be recognizable as Generalized Paxos since its message pattern and control flow remain the same. However, we chose to describe it in detail, both in the interest of clarity and also so that in the description of later iterations of the protocol we can focus on the aspects that differ. 

\begin{algorithm}[h] 
	\caption{Generalized Paxos - Proposer p}
	\textbf{Local variables:} $ballot\_type = \bot, ballot = 0 $
	\begin{algorithmic}[1]
		
		\State \textbf{upon} receive($BALLOT, bal, type$) \textbf{do} 
		\State \hspace{\algorithmicindent} 
		$ballot = bal$;
		\State \hspace{\algorithmicindent} 
		$ballot\_type = type$;
		\State
		
		\State \textbf{upon} command\_request($c$) \textbf{do}   \hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent}\# receive request from application
		\State \hspace{\algorithmicindent} \textbf{if} $ballot\_type = fast\_ballot$ \textbf{then}
		
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} \Call{send}{$P2A\_FAST, ballot, c$} to acceptors;
		\State \hspace{\algorithmicindent} \textbf{else} 
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} \Call{send}{$PROPOSE, c$} to leader;		
	\end{algorithmic}
\end{algorithm}

\subsection{Agreement Protocol} 

The consensus protocol allows learner processes to agree on equivalent sequences of commands (according to our previous definition of equivalence).
An important conceptual distinction between the original Paxos protocol and Generalized Paxos is that, in the original Paxos, each instance of consensus is called a ballot and agrees upon a single value, whereas in Generalized Paxos, instead of being separate instances of consensus, ballots correspond to an extension to the sequence of learned commands of a single ongoing consensus instance.
In both protocols, ballots can either be \textit{classic} or \textit{fast}. \par
\begin{algorithm}
	\caption{Generalized Paxos - Process p}
	\begin{algorithmic}[1]
		
		\Function{merge\_sequences}{$old\_seq, new\_seq$}
		\State \textbf{for} $c$ \textbf{in} $new\_seq$ \textbf{do} 
		\State \hspace{\algorithmicindent} \textbf{if} $!\Call{contains}{old\_seq,c}$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent} $old\_seq =  old\_seq \bullet c$;

		\State \textbf{return} $old\_seq$;
		\EndFunction
	\end{algorithmic}
\end{algorithm}
In classic ballots, a leader proposes a single sequence of commands, such that it can be appended to the commands learned by the learners. 
A classic ballot in Generalized Paxos follows a protocol that is very similar to the one used by classic Paxos~\cite{Lamport:1998}. This protocol comprises a first phase where each acceptor conveys to the leader the sequences that the acceptor has already voted for (so that the leader can resend commands that may not have gathered enough votes).
This is followed by a second phase where the leader picks an extension to the sequence of previously proposed commands and broadcasts it to the acceptors. The acceptors send their votes to the learners, who then, after gathering enough support for a given extension to the current sequence, append the new commands to their own sequences of learned commands and discard the already learned ones.\par

In fast ballots, multiple proposers can concurrently propose either single commands or sequences of commands by sending them directly to the acceptors. (We use the term \textit{proposal} to denote either the command or sequence of commands that was proposed.)
In this case, concurrency implies that acceptors may receive proposals in a different order. If the resulting sequences are equivalent, then the fast ballots are successfully learned in two message delays. If not, the protocol must fall back to using a classic ballot.

Next, we present the protocol for each type of ballot in detail.

\subsection{Classic Ballots} 

Classic ballots work in a way that is very similar to the original Paxos protocol~\cite{Lamport:1998}. Therefore, throughout our description, we will highlight the points where Generalized Paxos departs from the Classic Paxos protocol, in particular where it's due to behaviors caused by our simplified specification of Generalized Paxos.\par
In this part of the protocol, the leader continuously collects proposals by assembling commands received from the proposers in a sequence. This sequence is built by appending arriving proposals to a sequence containing every proposal received since the previous ballot. (This differs from classic Paxos, where it suffices to keep a single proposed value that the leader attempts to reach agreement on.)\par
When the next ballot is triggered, the leader starts the first phase by sending phase $1a$ messages to all acceptors containing just the ballot number. Similarly to classic Paxos, acceptors reply with a phase $1b$ message to the leader, which reports all sequences of commands they voted for. This message also implicitly conveys a promise not to participate in lower-numbered ballots, in order to prevent safety violations~\cite{Lamport:1998}.

\begin{algorithm} 
	\caption{Generalized Paxos - Leader l}
	\label{CFT-Lead}
	\textbf{Local variables:} $ballot_l = 0,proposals = \bot, accepted = \bot$
	\begin{algorithmic}[1]
		\State \textbf{upon} trigger\_next\_ballot($type$) \textbf{do}
		\State \hspace{\algorithmicindent} $ballot_l \mathrel{+{=}} 1$;
		\State \hspace{\algorithmicindent} $\Call{send}{BALLOT,ballot_l,type}$ to proposers;
		\State
		\State \hspace{\algorithmicindent} \textbf{if} $type = fast$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{FAST,ballot_l,view} $ to acceptors;
		\State \hspace{\algorithmicindent} \textbf{else}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{P1A, ballot_l, view}$ to acceptors;
		
		\State
		\State \textbf{upon} receive($PROPOSE, prop$) from proposer $p_i$ \textbf{do} 
		\State \hspace{\algorithmicindent} \textbf{if} $\Call{isUniversallyCommutative}{prop}$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{P1A, ballot, prop}$ to acceptors;
		\State \hspace{\algorithmicindent} \textbf{else}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $proposals = proposals \bullet prop$;
		\State
		\State \textbf{upon} receive($P1B, ballot, bal_a,vals_a$) from acceptor $a$ \textbf{do}
		\State \hspace{\algorithmicindent} \textbf{if} $ballot_a = ballot_l$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $accepted[ballot_l][a] =\langle bal_a, vals_a \rangle$;
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{if} $\#(accepted[ballot_l]) \geq N-f$ \textbf{then} 
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{phase\_2a}{ }$;
		
		\State
		\Function{phase\_2a}{$ $}	
		\State $votes = \bot$;
		\State $k = -1$;
		\State \textbf{for} $a$ \textbf{in} $acceptors$ \textbf{do}
		\State \hspace{\algorithmicindent} $bal_a = accepted[ballot_l][a][0]$;
		\State \hspace{\algorithmicindent} $val_a = accepted[ballot_l][a][1]$;
		\State \hspace{\algorithmicindent} \textbf{if} $bal_a > k$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $k = bal_a$;
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $votes = \bot$;
		\State \hspace{\algorithmicindent} \textbf{else if} $bal_a = k$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $votes[val_a] \mathrel{+{=}} 1$;
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{if} $votes[val_a] > f$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent} $maxTried_l = val_a$;
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent}\hspace{\algorithmicindent} \textbf{break};

		\State
		\State \textbf{for} $a$ \textbf{in} $acceptors$ \textbf{do}
		\State \hspace{\algorithmicindent} $maxTried_l = \Call{merge\_sequences}{maxTried_l, accepted[ballot_l][a]$};
		\State
		\State $maxTried_l = maxTried_l \bullet proposals$;
		\State $\Call{send}{P2A\_CLASSIC,ballot_l, maxTried_l}$ to acceptors;
		\State $proposals = \bot$;
		\State $maxTried_l = \bot$;
		\EndFunction	
	\end{algorithmic}
\end{algorithm}

After gathering a quorum of $N-f$ phase $1b$ messages, the leader initiates phase $2a$ by sending a message with a proposal to the acceptors. As was described in section \ref{value_picking}, the procedure followed by the leader to construct this proposal is critical to prevent conflicts between sequences proposed in different ballots as well as to ensure liveness even when conflicts occur during fast ballots. There are two possible scenarios when observing the quorum $Q$ of gathered phase $1b$ messages: either there is one reported sequence $s$ that was voted for at least $f+1$ acceptors in the latest ballot or there is none. If such a sequence exists then it's guaranteed to be the only one that may have been learned. Since $2f+1$ votes are necessary for any sequence to be learned and at least $f+1$ acceptors voted for $s$ then any other non-commutative sequence gathered at most $2f$ votes, which is insufficient for it to be learned. If no sequence in the quorum gathered $f+1$ votes then the leader can be sure that no value was or will be learned in that ballot. Since any sequence present in the quorum gathered at most $f$ votes and there are only $f$ acceptors outside of it, any sequence gathered at most $2f$ votes, which is also not enough for it to be learned. Therefore, we arrive at a well-defined value picking rule: given a quorum $Q$ of phase $1b$ messages, if some sequence $s$ has more than $f$ votes at the highest ballot in which some acceptor voted for, then that sequence is chosen as the prefix of the leader's proposal. If no such sequence exists, then the leader can assemble an arbitrary sequence composed of all the commands relayed in the phase $1b$ messages. In the first case, after picking the only sequence that may have been chosen, the leader appends commands present in other phase $1b$ messages that are not in the chosen sequence. This ensures liveness since any proposer's command that reaches more than $f$ acceptors before the next ballot begins will eventually be included in an accepted proposal. After executing this rule, the leader simply appends the proposers' commands to the sequence and sends it to the acceptors in phase $2a$ messages.\par

The acceptors reply to phase $2a$ messages by sending phase $2b$ messages to the learners, containing the ballot and the proposal from the leader. After receiving $N-f$ votes for a sequence, a learner learns it by extracting the commands that are not contained in his $learned$ sequence and appending them in order. Note that for a sequence to be learned, a learner doesn't have to receive $N-f$ votes for the exact same sequence but for equivalence sequences (in accordance to our previous definition of equivalence).

\subsection{Fast Ballots} 

In contrast to classic ballots, fast ballots are able to leverage a weaker specification of generalized consensus, in terms of command ordering at different replicas, to allow for faster execution of commands in some cases.

\begin{algorithm} 
	\caption{Generalized Paxos - Acceptor a}
	\label{CFT-Acc}
	\textbf{Local variables:} $leader = \bot, bal_a = 0,val_a = \bot,fast\_bal = \bot$
	\begin{algorithmic}[1]
		\State \textbf{upon} receive($P1A, ballot$) from leader \textbf{do}
		\State \hspace{\algorithmicindent} $\Call{phase\_1b}{ballot}$;
		
		\State
		\State \textbf{upon} receive($FAST,ballot$) from leader \textbf{do}
		\State \hspace{\algorithmicindent} $fast\_bal[ballot] = true$;
		
		\State
		\State \textbf{upon} receive($P2A\_CLASSIC, ballot, value$) from leader \textbf{do}
		\State \hspace{\algorithmicindent} $\Call{phase\_2b\_classic}{ballot, value}$; 
		
		\State		
		\State \textbf{upon} receive($P2A\_FAST,ballot,value$) from proposer p \textbf{do}
		\State \hspace{\algorithmicindent} $\Call{phase\_2b\_fast}{ballot, value}$;
		
		\State
		\Function{phase\_1b}{$ballot$}
		\State \textbf{if} $bal_a < ballot$ \textbf{then}
		\State \hspace{\algorithmicindent} \Call{send}{$P1B, ballot, bal_a, val_a$} to leader;
		\State \hspace{\algorithmicindent} $bal_a = ballot$;	
		\State \hspace{\algorithmicindent} $val_a = \bot$;	

		\EndFunction
		
		\State
		\Function{phase\_2b\_classic}{$ballot, value$}
		\State \textbf{if} $ballot \geq bal_a$ and $val_a = \bot$ \textbf{then}
		\State \hspace{\algorithmicindent} $bal_a = ballot$;
		\State \hspace{\algorithmicindent} \textbf{if} $\Call{isUniversallyCommutative}{value}$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{P2B, ballot, value}$ to learners;
		\State \hspace{\algorithmicindent}\textbf{else}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $val_a[ballot] = value$;
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{P2B, ballot, value}$ to learners;
		\EndFunction
		
		\State
		\Function{phase\_2b\_fast}{$ballot, value$}
		\State \textbf{if} $ballot = bal_a$ and $fast\_bal[bal_a]$ \textbf{then}
		\State \hspace{\algorithmicindent} \textbf{if} $\Call{isUniversallyCommutative}{value}$ \textbf{then}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{P2B, ballot, value}$ to learners;
		\State \hspace{\algorithmicindent}\textbf{else}
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $val_a[bal_a] =  \Call{merge\_sequences}{val_a[bal_a], value}$;
		\State \hspace{\algorithmicindent}\hspace{\algorithmicindent} $\Call{send}{P2B, bal_a, val_a[bal_a]}$ to learners;
		\EndFunction
	\end{algorithmic}
\end{algorithm}
The basic idea of fast ballots is that proposers contact the acceptors directly, bypassing the leader, and then the acceptors send their votes on proposals to the learners. If a learner can gather $N-f$ votes for a sequence (or an equivalent one), then it is learned. If, however, a conflict exists between sequences then they will not be considered equivalent and at most one of them will gather enough votes to be learned. Conflicts are dealt with by maintaining the proposals at the acceptors so they can be sent to the leader and learned in the next classic ballot. This differs from Fast Paxos where recovery is performed through an additional round-trip. \par
Next, we explain each of these steps in more detail.\par
\noindent {\bf Step 1: Proposer to acceptors.}
To initiate a fast ballot, the leader informs both proposers and acceptors that the proposals may be sent directly to the acceptors. Unlike classic ballots, where the sequence proposed by the leader consists of the commands received from the proposers appended to previously proposed commands, in a fast ballot proposals can be sent to the acceptors in the form of either a single command or a sequence to be appended to the command history. These proposals are sent directly from the proposers to the acceptors.\par

\begin{algorithm}
	\caption{Generalized Paxos - Learner l}
	\label{CFT-Learn}
	\textbf{Local variables: } $learned = \bot, messages = \bot$ 
	\begin{algorithmic}[1]
		\State \textbf{upon} receive($P2B, ballot, value$) from acceptor $a$ \textbf{do}
		\State \hspace{\algorithmicindent} $messages[ballot][value][a] = true$;
		\State \hspace{\algorithmicindent} \textbf{if} $\#(messages[ballot][value]) \geq N-f$ or ($\Call{isUniversallyCommutative}{value}$ and $\#(messages[ballot][value]) > f$) \textbf{then}
		\State \hspace{\algorithmicindent} \hspace{\algorithmicindent} $learned = \Call{merge\_sequences}{learned, value}$;
	\end{algorithmic}
\end{algorithm}

\noindent {\bf Step 2: Acceptors to learners.}
Acceptors append the proposals they receive to the proposals they have previously accepted in the current ballot and broadcast the result to the learners. Similarly to what happens in classic ballots, the fast ballot equivalent of the phase $2b$ message, which is sent from acceptors to learners, contains the current ballot number and the command sequence. However, since commands (or sequences of commands) are concurrently proposed, acceptors can receive and vote for non-commutative proposals in different orders. To ensure safety, correct learners must learn non-commutative commands in a total order. To this end, a learner must gather $N-f$ votes for equivalent sequences. That is, sequences do not necessarily have to be equal in order to be learned since commutative commands may be reordered. Recall that a sequence is equivalent to another if it can be transformed into the second one by reordering its elements without changing the order of any pair of non-commutative commands. (Note that, in Algorithm \ref{CFT-Lead} lines \{29-30\} and Algorithm \ref{CFT-Learn} lines \{2-3\}, equivalent sequences are being treated as belonging to the same index of the \textit{votes} or \emph{messages} variable, to simplify the presentation.) By requiring $N-f$ votes for a sequence of commands, we ensure that, given two sequences where non-commutative commands are differently ordered, only one sequence will receive enough votes. Since each acceptor will only vote for a single sequence, there are only enough correct processes to commit one of them. Note that the fact that proposals are sent as extensions of previous sequences is critical to the safety of the protocol. In particular, since the votes from acceptors can be reordered by the network before being delivered at the learners, if these values were single commands it would be impossible to guarantee that non-commutative commands would be learned in a total order. \par

\noindent \textbf{Arbitrating an order after a conflict.} When, in a fast ballot, non-commutative commands are  concurrently proposed, these commands may be incorporated into the sequences of various acceptors in different orders, and therefore the sequences sent by the acceptors in phase $2b$ messages will not be equivalent and will not be learned. In this case, the leader subsequently runs a classic ballot and gathers these unlearned sequences in phase $1b$. Then, the leader will assemble a single serialization for every previously proposed command, which it will then send to the acceptors. Therefore, if non-commutative commands are concurrently proposed in a fast ballot, they will be included in the subsequent classic ballot and the learners will learn them in a total order, thus preserving consistency. \par 
Note that the leader may assemble gathered sequences in an order that is non-commutative to some sequence that has previously gathered enough votes to be learned. Since the leader can only wait $N-f$ phase $1b$ messages from acceptors, there is no way to guarantee which sequence may have been learned. However, this is not a problem because, by making learners process phase $2b$ messages in order, if some sequence gathered enough votes to be learned then its commands will be learned before the new non-commutative sequence is and the new-sequence's repeated commands will be discarded.\par

\subsection{Discussion}

\subsubsection{Universally commutative commands}
% Generalized Paxos vs Paxos spec.
The Generalized Paxos specification dictates that the only commands whose relative order can be reversed at different learners are those that commute with each other. This restriction has the advantage of allowing for running a replicated state machine on top of the Generalized Paxos protocol, while still ensuring that the state machine behaves in a way that is indistinguishable from running it on top of the original Paxos protocol. However, the downside of this use of commutative operations in the context of Generalized Paxos is that this commutativity check is done at runtime, and, if non-commutative operations are issued concurrently, then we must fall back to the slower classic Paxos protocol.\par
This downside raises the possibility of extending the protocol to handle commands that are universally commutative, i.e., commute with every other command. For these commands, it is known before executing them that they will not generate any conflicts, and therefore it is not necessary to check them against concurrently executing commands. This allows us to optimize the protocol by decreasing the number of phase $2b$ messages required to learn to a smaller $f+1$ quorum. Since, by definition, these sequences are guaranteed to never produce conflicts, the $N-f$ quorum is not required to prevent learners from learning conflicting sequences. Instead, a quorum of $f+1$ is sufficient for the learner to be sure that the proposed sequence was proposed by a correct proposer. (Furthermore, for ``read-only'' commands, it is possible to run these against one single learner in the crash model, since there is no requirement to withstand $f$ faults for these commands.) \par
Even though proposers can propose universally commutative sequences that are guaranteed to never cause conflicts with any other sequence, in both classic and fast ballots, proposals are appended to a prefix of sequences that are guaranteed to have been or eventually be learned. However, the resulting sequence is unlikely to be universally commutative since it contains every previous sequence. This greatly diminishes the applicability of the overall optimization since it's only possible when every command in the history is universally commutative. {\color{red} Proposals are appended to proven sequences because, if they were sent individually, the network could reorder them and cause a conflict between multiple learners. However, since universally commutative commands are guaranteed to never cause conflicts, it's safe to send them individually and allow for them to be learned in arbitrary orders. When the leader or the acceptors receive universally commutative sequences, they immediately trigger the next phase without appending them to previous sequences. The reason why this is possible is because, since the proposal is a universally commutative sequence, it can arrive at the learner in a different order relative to other sequences at different learners while the final state will still converge across the system.\par
This optimization is particularly useful in the context of geo-replicated systems, since it can be significantly faster to wait for the $f+1$st message instead of the $N-f$th one. It can also be easily implemented with an additional check in some of the protocol's algorithms. This can be seen in Algorithm \ref{CFT-Lead} lines \{11-12\}, Algorithm \ref{CFT-Acc} lines \{24-25,34-35\} and Algorithm \ref{CFT-Learn} lines \{2-4\}.}

\subsubsection{Generalized Paxos and weak consistency}
%The Byzantine Generalized Paxos protocol tackles two challenges in two different avenues of research, fault tolerance and relaxed consistency models. By specifying the generalized consensus problem,
The key distinguishing feature of the specification of Generalized Paxos~\cite{Lamport2005} is allowing learners to learn concurrent proposals in a different order, when the proposals commute. This idea is closely related to the work on weaker consistency models like eventual or causal consistency~\cite{Ahamad1995}, or consistency models that mix
strong and weak consistency levels like RedBlue~\cite{Li2012}, which attempt to decrease the cost of executing operations by reducing coordination requirements between replicas. 

The link between the two models becomes clearer with the introduction of universally commutative commands in the previous subsection. In the case of weakly consistent replication (or multi-level replication), weakly consistent requests can be executed as if they were universally commutative, even if in practice that may not be the case. E.g., checking the balance of a bank account and making a deposit do not commute since the output of the former depends on their relative order. However, some systems prefer to run both as weakly consistent operations, even though it may cause executions that are not explained by a sequential execution, since the semantics are still acceptable given that the final state that is reached is the same and no invariants of the application are violated~\cite{Li2012}.
