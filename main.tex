
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}
\bibliographystyle{splncs03}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\usepackage{color}
\usepackage{url}
\usepackage{amsmath}
\usepackage[colorinlistoftodos,prependcaption,textsize=small]{todonotes}

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Replication with variable consistency and tolerance to arbitrary uncorrelated faults}
% a short form should be given in case it is too long for the running head
\titlerunning{Replication with variable consistency and tolerance to arbitrary uncorrelated faults}


\author{Miguel Pires \\
\texttt{miguel.pires@tecnico.ulisboa.pt}}
\authorrunning{Miguel Pires}

% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Instituto Superior Técnico (Campus Taguspark) \\
Av. Prof. Doutor Aníbal Cavaco Silva \\
2744-016 Porto Salvo - Portugal}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
The abstract should summarize the contents of the paper and should
contain at least 70 and at most 150 words. It should be written using the
\emph{abstract} environment.
\keywords{A few key words}
\end{abstract}

\section{Introduction}
\subsection{Motivation}

\section{Related Work}

Due to the specific characteristics of the scenario for which we try to optimize, there are several areas that contain works of interest to us. Subsection \ref{Paxos} describes previous work regarding consensus and several Paxos variants. Subsection \ref{Non-Crash} discusses non-crash fault models as well as protocols for these environments. Subsection \ref{Partial Synchrony} describes different partial synchrony models and how they fit in the synchrony spectrum. Subsection \ref{Variable Consistency} discusses variable consistency models that attempt to strike a balance between strong and weak consistency in order to obtain advantages from both. Despite the fact that the Paxos protocol family has been a subject of study for some time, this topic is now more relevant than ever since modern systems use these algorithms to implement high-availability, large scale systems. Subsection \ref{Coordination Systems} studies a few systems where these algorithms are used to enable fault-tolerance services.\todo{TODO: This whole paragraph needs to be rewritten after the introduction (for consistency)}

\subsection{Paxos and its Variants} \label{Paxos}

The Paxos protocol family solves consensus by circumventing the well-known FLP impossibility result \cite{Fischer1985}. It does this by making the observation that, while consensus is unsolvable in asynchronous systems, a system is usually synchronous since delays are sporadic and temporary. Therefore, as long as consistency is guaranteed regardless of synchrony, Paxos can forgo of progress during the temporary periods of asynchrony. Using this insight, Paxos ensures consistency even when the system is asynchronous but can only guarantee progress while it is synchronous and no more than $f$ faults occur for a system of $2f+1$ replicas \cite{Lamport2001}. The classic form of Paxos employs a set of proposers, acceptors and learners and runs in a sequence of ballots. To ensure progress during synchronous periods, proposals are serialized by a distinguished proposer, the leader. It is the leader that gathers proposals from other proposers and requests, in a \textit{phase 1a} message, that acceptors promise to not accept any ballots smaller than its own. Each acceptor responds to the leader's request, in a \textit{phase 1b} message, with either a promise or a message stating that it has already sent a promise to a proposer with a higher-numbered ballot. If the leader manages to obtain a promise from a quorum of acceptors, then it sends a \textit{phase 2a} message containing its proposal. The acceptors may accept the proposal and inform the learners with a \textit{phase 2b} message, if another proposer hasn't requested a promise for a higher-numbered ballot in the meanwhile. The fact that any proposer can request a promise and prevent lower ballots from being completed is the reason why Paxos can only ensure progress in a synchronous environment. Some proposer must be able to establish itself as the only leader in order to prevent proposers from continuously requesting promises for ballots with higher numbers. \par
We now describe the form in which Paxos is most commonly deployed, Multi(Decree)-Paxos. Multi-Paxos provides an optimization of the basic Paxos' message pattern by omitting the first phase of messages from all but the first ballot for each leader \cite{Renesse2011}. This means that a leader only needs to send a \textit{prepare} message once and subsequent proposals may be sent directly in \textit{phase 2a} messages, without requesting a promise from the acceptors. This reduces the message pattern in the common case from five message delays to just three (from proposal to learning). Since there are no implications on the quorum size or guarantees provided by Paxos, the reduced latency comes at no additional cost. \par
Fast Paxos observes that it's possible to improve on previous results by allowing proposers to propose values directly to acceptors. To this end, the protocol distinguishes between fast and classic ballots, where fast ballots use $3f+1$ acceptors instead of the usual $2f+1$ \cite{Lamport2006}. The optimized scenario occurs during fast ballots, in which only two messages broadcasts are necessary: \textit{phase 2a} messages between a proposer and the acceptors, and \textit{phase 2b} messages between acceptors and learners. This creates the possibility of two proposers concurrently proposing values to the acceptors and generating a conflict. The cost of dealing with this conflict depends on the chosen method of recovery, coordinated or uncoordinated, and if the leader or acceptors are also learners. The leader may choose  the recovery method in the beginning of the ballot. If coordinated recovery is chosen in a ballot $i$, then the cost is a \textit{phase 2} of round $i+1$ (i.e., two additional message delays). If uncoordinated recovery is chosen then the cost is a single \textit{phase 2b} message in round $i+1$ (i.e., one message delay) \cite{Lamport2006}. These numbers assume that either the leader (coordinated recovery) or the acceptors (uncoordinated recovery) are learners, which is a reasonable assumption. \par
Generalized Paxos addresses Fast Paxos' shortcomings regarding collisions. More precisely, it allows acceptors to accept different sequences of commands as long as non-commutative operations are totally ordered \cite{Lamport2005}.  Non-commutativity between operations is generically represented as a dependence relation. Generalized Paxos abstracts the traditional consensus problem of agreeing on a single value to the problem of agreeing on an increasing set of values. \textit{C-structs} provide this increasing sequence abstraction and allow us to define different consensus problems. If we define the sequence of learned commands of a learner $l_i$ as a \textit{c-struct} $learned[l_i]$, then the consistency requirement for consensus can be defined as:\par
\textbf{Consistency} $learned[l_1]$ and $learned[l_2]$ are always compatible, for all learners $l_1$ and $l_2$. \par
For two \textit{c-structs} to be compatible, they must have a \textit{common upper bound}. Plainly spoken this means that, for any two \textit{c-structs} such as $learned[l_1]$ and $learned[l_2]$, there must exist some \textit{c-struct} to which they are both prefixes. If  Defining \textit{c-structs} as command histories enables acceptors to agree on different sequences of commands and still preserve consistency as long as dependence relationships are not violated. This means that commutative commands can be ordered differently regarding each other but interfering commands must preserve the same order across each sequence at any learner. This guarantees that solving the consensus problem for histories is enough to implement a state-machine replicated system. \par
Mencius is also a variant of Paxos that tries to address the bottleneck of having a single leader through which every proposal must go through. In Mencius, the leader of each round rotates between every process. The leader of round $i$ is the process $p_k$, such that $k = n mod\ i$.  Leaders with nothing to propose can skip their turn by proposing a \textit{no-op}. If a leader is slow or faulty, the other replicas can execute \textit{phase 1} to revoke the leader's right to propose a value but they can only propose a \textit{no-op} instead \cite{Mao2008}. Considering that non-leader replicas can only propose \textit{no-ops}, a \textit{no-op} command from the leader can be accepted in a single message delay since there is no chance of another value being accepted. If some non-leader server revokes the leader's right to propose and suggests a \textit{no-op}, then the leader can still suggest a value $v \neq$ \textit{no-op} that will eventually be accepted as long as $l$ isn't permanently suspected. The usage of an unreliable failure detector is enough to guarantee that eventually all faulty processes, and only those, will be suspected. Mencius also takes advantage of commutativity by allowing out-of-order commits, where values $x$ and $y$ can be learned in different orders by different learners if there isn't a dependence relationship between them. Experimental evaluation confirms that Mencius is able of scaling to a higher throughput than Paxos. \par
Egalitarian Paxos (EPaxos) extends Mencius' goal of achieving a better throughput than Paxos by removing the bottleneck caused by having a leader. Additionally, EPaxos also achieves optimal commit latency in certain conditions and graceful performance degradation if replicas fail or become slow \cite{Moraru2013}. To avoid choosing a leader, the proposal of commands for a command slot is done in a decentralized manner. Each replica can propose a command with ordering constraints attached so that interfering commands can be totally ordered. In this way, EPaxos takes advantage of the commutativity observations made by Generalized Paxos that state that commutative commands may be ordered differently at different replicas without losing state convergence \cite{Lamport2005}. If two replicas unknowingly propose commands concurrently, one will commit its proposal in one round trip after getting replies from a quorum of replicas. However, some replica will see that another command was concurrently proposed and may interfere with the already committed command. If the commands are commutative then there is no need to impose a particular order but, if they're not commutative, then the replica must reply with a dependency between the commands. If a replica proposes a command $A$ first in a single round-trip then a the replica that tries to concurrently propose a non-commutative command $B$ would receive a reply with the ordering $B \rightarrow \{A\}$, to indicate that $B$ should be committed after $A$. In this scenario, committing command $B$ would require two round trips since the proposer would need to send another message to inform the other replicas of the new ordering constraint. This commit latency is achieved by using a \textit{fast-path quorum} of $f+\lfloor\frac{f+1}{2}\rfloor$ replicas which is the same as the \textit{slow-path quorum} of $f+1$ replicas, for $f \leq 2$. Similarly to Mencius, EPaxos achieves a substantially higher throughput than Multi-Paxos, especially when $2\%$ of commands interfere with each other. However, since, unlike Mencius, replicas don't have to wait for previous replicas to propose their commands, commit latency is significantly lower.

\subsection{Non-crash Fault Models} \label{Non-Crash}

Non-crash fault models emerged to cope with the effect of malicious attacks and software errors. These models (e.g., the arbitrary fault model) assume a stronger adversary than previous crash fault models. The Byzantine Generals Problem is defined as a set of Byzantine generals that are camped in the outskirts of an enemy city and have to coordinate an attack. Each general can either decide to attack or retreat and there may be $f$ traitors among the generals that try to prevent the loyal generals from agreeing on the same action. The problem is solved if every loyal general agrees on what action to take \cite{Lamport1982}. Like the traitorous generals, the Byzantine adversary is one that may force a faulty replica to display arbitrary behaviour and even coordinate multiple faulty replicas in an attack. \par
PBFT is a protocol that solves consensus for state machine replication (SMR) while tolerating up to $f$ Byzantine faults \cite{Castro1999}. The system moves through configurations called \textit{views} in which one replica is the primary and the remaining replicas are the backups. The safety property of the algorithm requires that operations be totally ordered. The protocol starts when a client sends a request for an operation to the primary, which in turn assigns a sequence number to the request and multicasts a \textit{pre-prepare} message to the backups. This message contains the timestamp, the message's digest, the view and the actual message. If a backup replica accepts the pre-prepare message, after verifying that the view number and timestamp are correct, it multicasts a \textit{prepare} message to and adds both messages to its log. The prepare message is similar to the pre-prepare message except that it doesn't contain the client's request message. Both of these phases are needed to ensure that the requested operation is totally ordered at every correct replica. The protocol's safety property requires that the replicated service must satisfy linearizability, and therefore operations must be totally ordered. After receiving $2f$ prepare messages, a replica multicasts a \textit{commit} message and commits the message to its log when it has received $2f$ commit messages from other replicas. The liveness property requires that clients must eventually receive replies to their requests, provided that there are at most $\lfloor\frac{n-1}{3}\rfloor$ faults and the transmission time doesn't increase continuously. This property represents a weak liveness condition but one that is enough to circumvent Fischer, Lynch and Paterson's impossibility result \cite{Fischer1985}. A Byzantine leader may try to prevent progress by omitting pre-prepare messages when it receives operation requests from clients, but backups can trigger new views after waiting for a certain period of time. \par
The cost of $3f+1$ replicas in a PBFT system stems from the immense power given to the adversary. The cross fault tolerance (XFT) model claims that this adversary is too strong when compared to the behavior of deployed systems \cite{Liu2015}. XFT assumes that machine and network faults are uncorrelated and this assumption allows it to have a cost equal to crash fault tolerant systems, $2f+1$, while still tolerating $f$ non-crash faults. The safety condition for this model is that correct replicas commit requests in a total order, as long as the system is not in anarchy. A system is in anarchy if there are any non-crash faults $f_{nc}(s)$ and the sum of crash faults $f_c(s)$, non-crash faults $f_{nc}(s)$ and partitioned replicas $f_p(s)$ is larger than the fault threshold. That is, the system is in anarchy if $f_{nc}(s) > 0\ \land\ f_{nc}(s)+ f_c(s)+f_p(s) > f$. Liveness is also guaranteed as long as the system is outside of anarchy. XFT replicates client requests to $f+1$ replicas synchronously while the remaining $f$ may learn about requests through lazy replication \cite{Ladin1992}. If an active replica fails, a view change is triggered. A view change requires all the replicas in the new view to collect the recent state, including the commit log, which may be substantial. This overhead can be decreased through the use of checkpointing. The protocol includes the exchange of four rounds of messages: SUSPECT, VIEW-CHANGE, VC-FINAL and NEW-VIEW. The experimental evaluation performed by the authors shows that, after each crash, the system triggers a view change that last less than 10 seconds. \par
The creators of the Visigoth Fault Tolerance (VFT) model also make the observation that the Byzantine model is unrealistic, especially for environments such as data centers. The Byzantine adversary's strength forces protocols to incur in an unnecessary cost of $3f+1$ replicas. VFT also notes that while data centers rarely observe arbitrary coordinated faults, they commonly experience data corruption faults (e.g., bit flips) and these also represent a type of arbitrary behaviour \cite{AmazonS31}\cite{AmazonS32}. To reconcile the need to tolerate different types of arbitrary faults, VFT observes that it is very unlikely for data corruption to affect the same data across multiple replicas. Therefore the key distinguishing factor between these types of arbitrary faults is \textit{correlation}. The VFT model proposes a customizable model that defines a limit of $u$ faults, that bounds the number of allowed omission (i.e., crash) and comission (i.e., arbitrary) faults, a limit of $o$ correlated comission faults, a limit of $r$ arbitrary faults and, for every process $p_i$, $s$ correct processes $p_j$ that are slow with respect to it \cite{Porto2015}. These additional assumptions allow VFT to decrease the replication factor from $n \geq 2u+r+1$ (i.e., $3f+1$ in the traditional notation) to $n \geq u+s+o+1$ (i.e., $f+s+o+1$), when $u > s$. These parameters allow the system administrator to configure the system to tolerate any combination of faults. The authors also propose a VFT state machine replication protocol, which is adapted from the BFT-SMaRT protocol and contains two main message patterns: the first consists in two message steps that are executed during epoch changes to collect information about previous epochs and disseminate this information to replicas; and the second consists in two multicast phases that are executed for each client request to implement the state machine replication. To gather a quorum, the gatherer tries to collect a larger quorum of $n-s$ replies, which can be seen as collecting replies from the fast but potentially faulty replicas. If a timeout occurs, the quorum is reduced to $n-u$ and this can be seen as collecting replies from correct but possibly slow replicas. This reduced quorum size is enough to ensure an intersection because since $x$ replicas didn't reply by the first timeout and only $s$ may be slow, then $x-s$ replicas must have crashed and won't participate in future quorums. The experimental evaluation confirms that VFT can tolerate uncorrelated arbitrary faults with resource-efficiency and performance that resembles that of CFT systems instead of BFT. \par
Dijkstra proposed self-stabilizing systems as a solution to the problem of keeping cyclic sequences of processes in a legitimate state even after the occurrence of transient faults \cite{Dijkstra1974}. In a self-stabilizing system, each process may contain several privileges and may only exchange information with its neighbors. The requirement of keeping the system in a legitimate state is met if: (1) one or more privileges are present; (2) in each state, the possible moves transfer the system to a legitimate state again (3) each privilege exists in at least one legitimate state; and (4) for any pair of legitimate states, there is a sequence of moves that transfers the system from one to the other. These systems define a set of rules for transferring privileges in a way such that a system that starts from an arbitrary state is guaranteed to converge to a legitimate state and remain in a set of legitimate states. Self-stabilizing systems are an example of a non-crash model that isn't strictly stronger than crash fault models since, due to the system's ring-like structure, a crash fault would render the system unusable. A self-stabilizing system tolerates any arbitrary corruption of the state but, unlike other non-crash models, it assumes that a process always executes correct code.\par
The Arbitrary State Corruption (ASC) model defines ASC faults as faults that occur at a process $p$ and either make it crash or assign an arbitrary value to a variable of the process's state. The assignment of an arbitrary value to a variable of $p$'s state may also alter its control flow in a way such that the process may execute any of its instructions \cite{Correia2012}. This modeling stems from three main observations: (1) most tolerable failures are caused by state corruptions instead of arbitrary or malicious behavior, (2) control-flow faults are very likely to cause incorrect outputs \cite{Kalbarczyk2003}, and (3) the relevant failures are caused by transient faults instead of permanent faults being repeatedly activated. In the context of this model, the ASC hardening technique is proposed to deal with ASC faults. This technique guarantees that if a correct hardened process $p_c$ receives a message $m$ from a faulty hardened process $p_{f1}$, then $p_c$ discards $m$ without modifying its state. If a faulty process $p_{f2}$ receives $m$ and modifies its local state, then it crashes before sending an output message. ASC hardening uses message integrity checks to prevent network corruption and state integrity checks to replicate process state. Every time a value is read from a variable, it's checked against the replicated value. If a mismatch occurs, the process crashes itself to preserve integrity. Control-flow faults are prevented through the use of gates, which are sequences of instructions that ensure that a portion of code is executed the appropriate amount of times. For instance, we may want to ensure that an event handler executes exactly once per event. For each control-flow gate, two variables and a label are created, respectively, $c$, $c'$ and $L$. A gate executes a sequence of checks and assignments to ensure the required semantics. A useful type of control-flow gate is an \textit{at-most-once} gate, which ensures that code isn't executed twice even in the presence of faults. An \textit{at-most-once} gate implements this semantic by crashing the process if $c \neq c' \vee c=L$. In the first execution of the code, the condition fails, both $c$ and $c'$ are set to $L$ and the execution continues. In a second execution, the condition would evaluate to true since $c$ has already been set to $L$ and the process would crash. The $c \neq c'$ condition prevents an arbitrary state fault from corrupting the variable's value. Another relevant type of control-flow gate is an \textit{at-least-once} gate, which ensures that a gate has been executed before. For an \textit{at-least-once} gate, if $c=c'=L$ the gate has been reached before. Otherwise, the process crashes itself. ASC is similar to self-stabilizing systems \cite{Dijkstra1974} in the sense that both assume that only state can become corrupted, while the executed code is always correct. An ASC-hardened implementation of Paxos has a throughput up to 70\% higher than PBFT.

\todo{Should I mention Zeno?}
\subsection{Partial Synchrony} \label{Partial Synchrony}

A failure detector is a formalism that allows us to abstract the assumptions a system is allowed to make about its environment. Unreliable failure detectors were proposed by Chandra and Toueg as a way to circumvent impossibility results due asynchrony. These failure detectors would output information about which processes have crashed, therefore encapsulating the necessity of determining whether a process is crashed or slow \cite{DeepakChandra1996}. The guarantees provided by the failure detector are specified in terms of \textit{completeness}, which requires that a failure detector must eventually suspect every crashed process, and \textit{accuracy}, which restricts the mistakes a failure detector can make. A class of failure detectors of particular interest is Eventually Weak failure detectors, $\Diamond\mathcal{W}$, since it is the weakest of all classes defined by Chandra and Toueg. $\Diamond\mathcal{W}$ failure detectors provide the following guarantees:\par
\textbf{Completeness} Eventually every crashed process is permanently suspected by some correct process.\par
\textbf{Accuracy} Eventually some correct process is never suspected by any correct process.\par
Chandra and Toueg prove that consensus is solvable using a $\Diamond\mathcal{W}$ failure detector with $f < \lceil n/2 \rceil$ \cite{DeepakChandra1996}. Chandra et al. also define a new class of failure detectors, $\Omega$-accurate failure detectors \cite{Chandra1996}. The $\Omega$ class is shown to be at least as strong as $\Diamond\mathcal{W}$ and any failure detector that allows consensus to be solved must be at least as strong as $\Omega$. Therefore, any failure detector that allows consensus to be solved is at least as strong as $\Diamond\mathcal{W}$ which is then the weakest possible class of failure detector with which consensus can be solved. \par
The accuracy properties of the failure detectors proposed by Chandra and Toueg in \cite{DeepakChandra1996} must apply to all processes in the system. If a network partition occurs, the accuracy property can violated since correct processes in one partition may be suspected to have failed by processes in the a different  partition. $\Gamma$-accurate failure detectors were proposed by Guerraoui and Schiper \cite{Guerraoui96gammaaccurate} to deal with this limitation in the specification of $\Omega$-accurate failure detectors. $\Gamma$-accurate failure detectors apply the completeness and accuracy properties to a subset of the system's processes and allow consensus to be solved in a fully asynchronous model. The accuracy property for a $\Gamma$-accurate Eventually Weak failure detector, $\Diamond\mathcal{W}(\Gamma)$, becomes: \par
\textbf{Accuracy} Eventually some correct process (not necessarily in $\Gamma$) is never suspected by any process in $\Gamma$.\par
Since $\Gamma$-accurate Eventually Weak failure detectors are strictly weaker than $\Omega$-accurate Eventually Weak failure detectors \cite{Guerraoui96gammaaccurate}, $\Diamond\mathcal{W}(\Gamma) \prec \Diamond\mathcal{W}$, and  $\Diamond\mathcal{W}$ failure detectors have been shown to be the weakest class of failure detectors for which consensus is solvable \cite{Chandra1996}, then it follows that $\Diamond\mathcal{W}(\Gamma)$ failure detectors do not allow for consensus to be solved.\par
Wide-area systems are often considered to be asynchronous since they cannot provide a fixed upper bound $\Delta$ on the time messages take to be delivered (communication synchrony) or a fixed upper bound $\Phi$ on the drift rate between processes' clocks (process synchrony). However, Dwork, Lynch and Stockmeyer note that one reasonable situation is when there exists an upper bound $\Delta$ on message delivery latency but it isn't known \textit{a priori} \cite{Dwork1988}. In a realistic deployment, a system could consider the network to be synchronous for most of the time since connectivity and network problems are sporadic. In this situation, the impossibility results shown in \cite{Fischer1985} and \cite{Dolev1983} don't apply since communication can be considered synchronous. To take advantage of an existent but unknown upper bound $\Delta$ on communication time, a consensus-solving protocol can't explicitly use $\Delta$ as a timeout value because it's unknown. Additionally, for this protocol to progress, there should be a time after which this upper bound holds. Therefore, an additional constraint is applied to the model: there is a global stabilization time (GST) after which communication latency is bounded by $\Delta$. An algorithm that solves consensus is required to ensure \textit{safety} regardless of how asynchronous the system behaves, but it only needs to ensure \textit{termination} if $\Delta$ eventually holds. To extend the model to allow partially synchronous processes, it's possible to modify the additional constraint to: there is a global stabilization time (GST) after which communication latency and relative processor speed are by bounded by $\Delta$ and $\Phi$, respectively. The authors also show that f-resilient consensus is possible in the partially synchronous model if $N \geq 2f+1$, for crash faults, and $N \geq 3f+1$, for Byzantine faults. These lower bounds are the same regardless of whether processors are considered synchronous or partially synchronous.\par
Aguilera et al. propose a partial synchrony model based on the notion of \textit{set timeliness} and show that it can be used to study problems weaker than consensus \cite{Aguilera2012}. This model generalizes the concept of \textit{process timeliness} (i.e., an upper bound $\Phi$ on the relative speeds of processes) \cite{Dwork1988} to sets of processes by considering a set of processes as a single virtual process that executes an action whenever a process in $P$ executes an action and applying the definition of process timeliness to virtual processes. A set of processes $P$ is considered to be timely with respect to another set of processes $Q$ if, for some integer $i$, every interval that contains $i$ steps in $Q$ also contains at least one step of a process in $P$. This model is used to prove possibility and impossibility results for the $f$-resilient $k$-set agreement problem. This problem is a generalization of the consensus problem where, instead of having $n$ processes that have to agree on a single value, they need to decide on at most $k$ different values \cite{Chaudhuri1993}. To solve this problem, Aguilera et al. propose a $f$-resilient $k$-anti-$\Omega$ failure detector that, for every process $p$, outputs a set $fdOutput_p$ such that there exists a correct process $c$ that eventually doesn't belong to $fdOutput_p$. Anti-$\Omega$ failure detectors were shown to be the weakest failure detectors that allow set agreement to be solved \cite{Zielinski2010}. The algorithm works for two sets $P$ and $Q$ of sizes $k$ and $f+1$, respectively, such that $P$ is timely with respect to $Q$. In this $f$-resilient $k$-anti-$\Omega$ failure detector, each process has a periodically incremented heartbeat and also timeout timers for every set $A$, such that $A$ is a subset of all processes $\Pi_n$ of size $k$. Whenever a process detects that \textit{any} process in some set $A$ incremented its heartbeat, it resets $A$'s timer. If $A$'s timer expires for a process $p$, $p$ increments its timeout value for $A$ and also increments a shared register $Counter[A,p]$ that represents $A$'s untimeliness with respect to $p$. An accusation counter of a set $A$ is the $(f+1)$-st smallest value of $Counter[A,*]$. Each process $p$ picks the set that has the smallest accusation counter as its $winnerset_p$ and sets $\Pi_n - winnerset_p$ as the output of $k$-anti-$\Omega$. Since $P$ is timely with respect to $Q$, eventually the accusation counter of $P$ stops changing and one of the sets whose accusation counter stops changing is the smallest and is picked as the winner. \par

\subsection{Variable Consistency Models} \label{Variable Consistency}
Replicating state across geographically separate datacenters requires us to reason about consistency. Traditionally, consistency models could be grouped into two categories: weak and strong. Strong consistency model, such as serializability \cite{Herlihy1990}, ensure a total order of operations across all replicas and provide familiar one-copy semantics but incur in an unacceptable latency cost for many applications. Weak consistency models, such as causal consistency \cite{Ahamad1995}, address this flaw by relaxing the order guarantees that are ensured and allowing operations to be concurrently executed without coordination between replicas. A smaller subset of this category is eventual consistency where ordering guarantees are relaxed and state divergence is allowed as long as it is reconciled. Other models attempt to strike a balance between these two extremes. Timeline consistency ensures that all replicas of a record apply updates in the same order by forwarding updates to a master. However, reads are executed at any replica and return consistent but possibly stale data \cite{Silberstein2008}. Variable consistency models emerged to address the tension between weak and strong consistency by relaxing ordering guarantees for commutative operations while requiring non-commutative operations to be totally ordered across all replicas. \par
RedBlue is a variable consistency model that allows two consistency levels to coexist, red and blue. Blue operations may be executed in different orders at different replicas and are considered to be fast while red operations must be totally ordered across all replicas and are considered slow \cite{Li2012}. For a RedBlue system to remain in a valid state, pairs of non-commutative operations and operations that may violate invariants should be totally ordered. Taking the example of a banking system, since the \textit{withdraw} operation may violate the invariant that the balance should not be less than zero, it must be totally ordered to ensure state convergence. Therefore, an operation $u$ must be labeled red if it may result in an invariant being violated or if there is another operation $v$ that is non-commutative with $u$. All other operations may be labeled as blue. To maximize the amount of operations that can be labeled as blue, RedBlue notes that while operations themselves may not be commutative, it's often possible to make their updates to the system's data commute. To do this, operations are divided into two parts: a generator operation and a shadow operation. Generator operations compute the changes an operation makes to the state without any side-effect and are only executed in the site in which they are submitted. Shadow operations apply the changes computed by the corresponding generator operation and are executed at all sites. This allows some operations like \textit{deposit} and \textit{accrueInterest} to be treated as commutative by first calculating the amount of interest to be deposited and then treating that amount as a deposit.  However, it's important to note that the side-effects are computed at the site where the operation was submitted and, therefore, other replicas might see unexpected changes in their local view of the system's state. Continuing the previous example with a banking account replicated in two sites, if an \textit{accrueInterest(5\%)} operation is submitted while one site has a balance of \$100 and the other has a balance of \$200, due to a concurrent \textit{deposit(100)} operation, then depending on which site the operation is submitted the interest deposited would be either \$5 or \$10 at both sites. This makes the system convergent but seems anomalous in terms of user experience.\par
The Explicit Consistency model also strives to preserve as much concurrency as possible while preserving consistency when needed. This model guarantees that application-specific invariants are maintained by detecting and handling sets of operations that may cause invariants to be violated, \textit{I-offender sets} \cite{Balegas2015}. Indigo is a middleware layer that implements this model and its approach consists of three steps: (1) detecting \textit{I-offender sets}, (2) choosing either an invariant repair or violation avoidance technique for handling these sets and (3) instrumenting the application code to use the chosen mechanism. The discovery of \textit{I-offender sets} is performed through static analysis of operation post-conditions and application invariants, which are both specified by the application developer in first-order logic. This approach allows the programmer to define fine-grained consistency semantics which reduce ordering constraints between operations. As previously mentioned, to deal with conflicting sets of operations, the developer may select either invariant repair or violation avoidance techniques. Indigo provides a library of objects (e.g., sets, maps, trees) that repair invariants according to different conflict resolution policies which the programmer can extend to support additional invariants. Indigo also provides reservation techniques that avoid the concurrent execution of possibly conflicting operations. The base mechanism of these techniques are multi-level lock reservations which provide replicas with \textit{shared forbid}, \textit{shared allow} or \textit{exclusive allow} rights to execute an action. Whenever a replica holds a certain type of right, no other replica can hold rights of a different kind. Multi-level locks allow for the enforcement of any type invariant but Indigo supports additional techniques that provide increased concurrency. For instance, multi-level mask reservations allow two conflicting operations to be concurrently executed if some other predicate remains true. This is useful for invariants like $P_1 \vee P_2 \vee ... \vee P_n$, where the concurrent execution of only two operations is not enough for the invariant to be executed. This type of reservation can be seen as a vector of multi-level locks where, if a replica obtains a shared allow in one entry, it must obtain a shared forbid in some other entry. 

\subsection{Coordination Systems} \label{Coordination Systems}

Zookeeper is a replicated coordination service for distributed applications. It maintains a consistent image of state across all replicas and totally orders updates such that subsequent operations can implement high-level abstractions such as leader election, synchronization and configuration maintenance. Zookeeper uses a primary-backup scheme and an atomic broadcast algorithm called Zab that is tuned specifically for Zookeeper's setting \cite{Junqueira2011}. This protocol functions similarly to Paxos but guarantees that multiple outstanding operations are committed in FIFO order. The reason why this additional guarantee is necessary is because state changes are incremental with respect to the previous state, so there is an implicit dependence relation on the order of operations. In Paxos, even if operations are proposed in a batch, leader failures may cause proposals to be committed in an order different from what the client proposed. The Zab protocol functions in three phases: (1) Discovery, (2) Synchronization and (3) Broadcast. In the  first two phases, the leader obtains the longest history of the latest epoch and sends the new history to the followers. These two phases guarantee that outstanding proposals are committed in FIFO order. In the broadcast phase, the leader sends proposals to the followers, who write the proposals to stable storage and reply with an acknowledgment. After receiving a quorum of acknowledgments, the leader sends a commit message $\langle v,z \rangle$ to indicate that the followers should deliver all undelivered proposals that are causally related to $z$, $z' \prec z$. \par
Chubby is a lock service that provides reliable, advisory locking functionality for loosely-coupled distributed systems. Chubby exposes both a namespace and an interface that resemble a distributed file system and through which clients acquire and release locks \cite{Burrows2006}. A Chubby cell consists of a small set of replicas where one of them functions as a master. The master is elected through a consensus protocol by obtaining a quorum of votes. The master must also obtain promises that the replicas won't elect a different master for a time period known as the \textit{master lease}. Clients send master location requests to the replicas listed in the DNS and non-master replicas respond with the master's identity. Once a client knows the master's location it will direct all requests to that location. Write requests are propagated to the replicas through Paxos and the leader sends an acknowledgment to the client when the write reaches a quorum. Read requests can be serviced by the master alone because, as long as the master lease has not expired, no other master can exist and issue write requests.

\bibliography{references}

\end{document}
